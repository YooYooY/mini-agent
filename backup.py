from __future__ import annotations
from typing import TypedDict, Literal, Optional, List, Dict
import uuid
import json
import os

import dotenv
from langgraph.graph import StateGraph, END
from vector_store_chroma import vector_index
from langchain_openai import ChatOpenAI


llm = ChatOpenAI(
    model="gpt-4.1-mini",
    temperature=0,
)

dotenv.load_dotenv()

# ========= ç±»å‹å®šä¹‰ =========


class IntentContext(TypedDict, total=False):
    topic: str
    intent: str
    task_plan: List[str]


class RetrievalContext(TypedDict, total=False):
    query: str
    doc_scope: List[str]
    retriever_hits: List[Dict]


class ExecutionTrace(TypedDict, total=False):
    step: str
    tool: str
    input: Dict
    output: Optional[Dict]
    status: str  # success / warning / error
    error: Optional[str]
    critic_round: int
    next_step: Optional[str]


class CriticResult(TypedDict, total=False):
    status: Literal["pass", "revise", "fail"]
    reason: str
    critic_count: int
    action: Optional[str]  # e.g. "redo_retriever", "stop"


class TaskState(TypedDict, total=False):
    task_id: str
    user_query: str
    intent_context: IntentContext
    retrieval_context: RetrievalContext
    answer: str
    execution_trace: List[ExecutionTrace]
    critic_result: CriticResult
    resume_next_step: Optional[str]


# ========= å…¨å±€ Memory =========

memory_store: Dict[str, Dict] = {}

# ========= Checkpoint =========

CHECKPOINT_DIR = "./checkpoints"


def ensure_checkpoint_dir():
    os.makedirs(CHECKPOINT_DIR, exist_ok=True)


def checkpoint_path(task_id: str) -> str:
    return os.path.join(CHECKPOINT_DIR, f"{task_id}.json")


def save_checkpoint(task_id: str, state: TaskState, last_step: str, next_step: str):
    """ä¿å­˜å½“å‰ä»»åŠ¡ checkpointï¼ŒåŒ…æ‹¬ next_step"""
    ensure_checkpoint_dir()
    payload = {
        "task_id": task_id,
        "last_step": last_step,
        "next_step": next_step,
        "state": state,
        "memory": memory_store[task_id],
    }
    with open(checkpoint_path(task_id), "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)


def load_checkpoint(task_id: str):
    path = checkpoint_path(task_id)
    if not os.path.exists(path):
        return None
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def has_checkpoint(task_id: str) -> bool:
    return os.path.exists(checkpoint_path(task_id))


# ========= Trace å·¥å…· =========


def append_trace(
    task_id: str,
    step: str,
    tool: str,
    input_data: Dict,
    output: Optional[Dict] = None,
    status: str = "success",
    error: Optional[str] = None,
    next_step: Optional[str] = None,
):
    critic_round = memory_store[task_id]["critic_result"]["critic_count"]
    trace_item: ExecutionTrace = {
        "step": step,
        "tool": tool,
        "input": input_data,
        "output": output,
        "status": status,
        "error": error,
        "critic_round": critic_round,
        "next_step": next_step,
    }
    memory_store[task_id]["execution_trace"].append(trace_item)


# ========= ä»»åŠ¡åˆå§‹åŒ– =========


def init_task_memory(task_id: Optional[str] = None) -> str:
    if task_id is None:
        task_id = str(uuid.uuid4())
    memory_store[task_id] = {
        "task_meta": {"task_id": task_id},
        "intent_context": {},
        "retrieval_context": {},
        "execution_trace": [],
        "critic_result": {
            "critic_count": 0,
            "status": "pass",
            "reason": "",
            "action": None,
        },
    }
    return task_id


def create_init_state(task_id: str, user_query: str) -> TaskState:
    return TaskState(
        task_id=task_id,
        user_query=user_query,
        intent_context={},
        retrieval_context={},
        answer="",
        execution_trace=[],
        critic_result={
            "critic_count": 0,
            "status": "pass",
            "reason": "",
            "action": None,
        },
    )


# ========= Node å®ç° =========


def entry_node(state: TaskState) -> TaskState:
    # å…¥å£ä¸åšäº‹ï¼Œä»…è´Ÿè´£äº¤ç»™ route_from_entry å†³å®šèµ·ç‚¹
    return state


def route_from_entry(state: TaskState) -> str:
    resume_next = state.get("resume_next_step")
    if resume_next:
        return resume_next
    return "planner_node"


def planner_node(state: TaskState) -> TaskState:
    task_id = state["task_id"]
    task_memory = memory_store[task_id]

    # ç®€å•åœ°ç”¨ user_query æ¨å‡º topic / intent
    user_query = state["user_query"]

    if "è®¢å•" in user_query or "order" in user_query.lower():
        topic = "è®¢å• API"
        intent = "æŸ¥è¯¢è®¢å•ç›¸å…³æ¥å£ä¿¡æ¯å¹¶ç”Ÿæˆç¤ºä¾‹ä»£ç "
    else:
        topic = "æœªçŸ¥ä¸»é¢˜"
        intent = f"æŸ¥è¯¢ä¸ã€Œ{user_query}ã€ç›¸å…³çš„ä¿¡æ¯"

    task_plan = ["æ£€ç´¢æ–‡æ¡£", "éªŒè¯é¢†åŸŸæ˜¯å¦åŒ¹é…", "ç”Ÿæˆå›ç­”"]

    intent_context: IntentContext = {
        "topic": topic,
        "intent": intent,
        "task_plan": task_plan,
    }

    task_memory["intent_context"] = intent_context
    state["intent_context"] = intent_context

    next_step = "retriever_node"

    append_trace(
        task_id=task_id,
        step="planner_node",
        tool="intent_planner",
        input_data={"user_query": user_query},
        output={"intent_context": intent_context},
        next_step=next_step,
    )

    save_checkpoint(task_id, state, last_step="planner_node", next_step=next_step)

    return state


def retriever_node(state: TaskState) -> TaskState:
    """
    ä½¿ç”¨ Chroma çš„çœŸå®å‘é‡æ£€ç´¢ï¼š
    - retriever ä¸å† mock
    - è¿”å›çœŸå® chunk
    - evidence å†™å…¥ trace
    - checkpoint è®°å½•çŠ¶æ€
    """
    task_id = state["task_id"]
    task_memory = memory_store[task_id]

    query = state["user_query"]

    hits = vector_index.search(query, k=3)

    retrieval_context: RetrievalContext = {
        "query": query,
        "doc_scope": ["orders", "api"],
        "retriever_hits": hits,
    }
    task_memory["retrieval_context"] = retrieval_context
    state["retrieval_context"] = retrieval_context

    # evidence è¿›å…¥ traceï¼Œåªå†™ç²¾ç®€ç‰ˆ
    evidence_preview = [
        {
            "doc_id": h["doc_id"],
            "title": h["title"],
            "score": h["score"],
        }
        for h in hits
    ]

    next_step = "executor_node"

    append_trace(
        task_id=task_id,
        step="retriever_node",
        tool="chroma_vector_retriever",
        input_data={"query": query},
        output={"hit_count": len(hits), "evidence": evidence_preview},
        next_step=next_step,
    )

    save_checkpoint(task_id, state, last_step="retriever_node", next_step=next_step)

    return state


def executor_node(state: TaskState) -> TaskState:
    task_id = state["task_id"]
    task_memory = memory_store[task_id]

    intent = task_memory["intent_context"].get("intent", "")
    hits = task_memory["retrieval_context"].get("retriever_hits", [])

    # ç”¨ evidence æ–‡æœ¬ç”Ÿæˆä¸€ä¸ªâ€œå‡å›ç­”â€ï¼ˆçœŸå®å·¥ç¨‹é‡Œæ¢æˆ LLM è°ƒç”¨ï¼‰
    doc_titles = [h["title"] for h in hits]
    answer = f"æ ¹æ®æ„å›¾ã€Œ{intent}ã€ï¼Œå¹¶å‚è€ƒæ–‡æ¡£ï¼š{', '.join(doc_titles)}ï¼Œç”Ÿæˆçš„ç¤ºä¾‹å›ç­”ï¼ˆè¿™é‡Œçœç•¥ LLM è°ƒç”¨ï¼‰ã€‚"

    state["answer"] = answer

    next_step = "critic_node"

    append_trace(
        task_id=task_id,
        step="executor_node",
        tool="answer_generator",
        input_data={"intent": intent, "hit_count": len(hits)},
        output={"answer_preview": answer[:80]},
        next_step=next_step,
    )

    save_checkpoint(task_id, state, last_step="executor_node", next_step=next_step)

    return state


# ======== åŸŸåŒ¹é…è§„åˆ™ï¼ˆæ¨¡æ‹Ÿâ€œè¯­ä¹‰â€æ£€æµ‹ï¼‰========


def _run_llm_critic(user_query: str, intent: str, hits: list[dict], draft_answer: str):
    """
    è¿”å›ä¸€ä¸ªç»“æ„åŒ–è£å†³ç»“æœï¼ˆLLM åŠ©æ‰‹è§’è‰² onlyï¼‰
    """

    evidence_text = "\n\n".join(
        f"[{h['title']}]\n{h['chunk']}\n(score={h['score']})"
        for h in hits[:3]  # æ§åˆ¶é•¿åº¦
    )

    prompt = f"""
You are a retrieval quality auditor for an AskMyDocs RAG system.

Task:
Evaluate whether the retrieved evidence is semantically relevant
to the user query and the planned task intent.

User Query:
{user_query}

Task Intent:
{intent}

Draft Answer (generated from evidence):
{draft_answer}

Retrieved Evidence Chunks:
{evidence_text}

You must reason carefully and output a structured JSON with fields:

- status:
  - "pass"   â†’ evidence matches query & task domain
  - "revise" â†’ evidence seems partially relevant or mismatched, redo retriever
  - "fail"   â†’ critically wrong retrieval or cannot recover

- reason:
  short natural language justification

- action:
  - "redo_retriever"
  - "stop"
  - null if status="pass"
"""

    resp = llm.invoke(prompt)

    # é¢„æœŸ LLM è¾“å‡º json æˆ–æ¥è¿‘ json
    import json

    try:
        result = json.loads(resp.content)
    except Exception:
        result = {
            "status": "revise",
            "reason": "LLM critic returned invalid response, fallback to revise",
            "action": "redo_retriever",
        }

    return result


def _retrieval_sanity_check(
    user_query: str, hits: list[dict]
) -> Literal[True, False, None]:
    """
    è¿”å›:
        True  -> æ˜ç¡®å¥åº·ï¼ˆå…è®¸ LLM critic å†³å®š pass/reviseï¼‰
        False -> æ˜ç¡®ä¸å¥åº·ï¼ˆè‡³å°‘ reviseï¼‰
        None  -> ä¸ç¡®å®šï¼ˆå®Œå…¨äº¤ç»™ LLM criticï¼‰

    âš ï¸ è¿™é‡Œä¸åšâ€œä¸šåŠ¡è¯­ä¹‰åˆ¤æ–­â€
    åªè´Ÿè´£ï¼šç³»ç»Ÿç¨³å®šæ€§ / åç›´è§‰ / ç»“æ„å¼‚å¸¸
    """

    # 1) å®Œå…¨æœªå¬å› â†’ è‡³å°‘ revise
    if len(hits) == 0:
        return False

    # 2) evidence å‡ä¸ºé‡å¤ chunk â†’ retriever æ˜æ˜¾å¼‚å¸¸
    chunks = [h["chunk"] for h in hits]

    print(
        "len(set(chunks)) == 1 and len(chunks) > 1",
        len(set(chunks)) == 1 and len(chunks) > 1,
    )

    if len(set(chunks)) == 1 and len(chunks) > 1:
        return False

    # 3) evidence è¿‡çŸ­ï¼ˆå¯èƒ½æ˜¯æ ‡é¢˜ / å™ªå£°ï¼‰
    if all(len(c) < 30 for c in chunks):
        return False

    # â­ï¸ æ­£å¸¸æƒ…å†µï¼šäº¤ç»™ LLM å†³å®š
    return None


def _merge_sanity_and_llm(rule_state: bool | None, llm_result: dict):
    """
    rule_state:
        True  -> evidence å¥åº·ï¼ˆLLM æœ‰æœ€ç»ˆå†³å®šæƒï¼‰
        False -> evidence æ˜æ˜¾å¼‚å¸¸ï¼ˆLLM ä¸èƒ½ç›´æ¥ PASSï¼‰
        None  -> äº¤ç»™ LLM critic

    é‡ç‚¹ï¼š
    - rule å…œåº• = åªé˜»æ­¢ LLMâ€œè¿‡åº¦è‡ªä¿¡ PASSâ€
    - ä¸å¼ºæ¨ PASS / ä¸åšä¸šåŠ¡æ¨æ–­
    """

    # evidence æ˜æ˜¾å¼‚å¸¸ â†’ è‡³å°‘ revise
    if rule_state is False:
        if llm_result.get("status") == "pass":
            return {
                "status": "revise",
                "reason": "sanity check failed (retriever unhealthy), avoid false-pass",
                "action": "redo_retriever",
            }
        return llm_result

    # æ­£å¸¸ / ä¸ç¡®å®š â†’ å®Œå…¨å°Šé‡ LLM è¯­ä¹‰ critic
    return llm_result


def critic_node(state: TaskState) -> TaskState:
    task_id = state["task_id"]
    task_memory = memory_store[task_id]

    hits = task_memory["retrieval_context"].get("retriever_hits", [])
    critic_prev = task_memory["critic_result"]
    critic_count = critic_prev["critic_count"]
    user_query = state["user_query"]

    intent = task_memory["intent_context"].get("intent", "")
    draft_answer = state.get("answer", "")

    # critic ä¸Šé™é˜²æ­»å¾ªç¯
    if critic_count >= 2:
        critic_result = {
            "status": "fail",
            "reason": "critic count exceeded",
            "action": "stop",
        }
    else:
        rule_state = _retrieval_sanity_check(user_query, hits)
        llm_result = _run_llm_critic(
            user_query=user_query,
            intent=intent,
            hits=hits,
            draft_answer=draft_answer,
        )

        critic_result = _merge_sanity_and_llm(rule_state, llm_result)

    # --- critic_count æ›´æ–°ç­–ç•¥ ---
    if critic_result["status"] == "pass":
        new_count = 0
    else:
        new_count = critic_count + 1

    critic_result = {
        **critic_result,
        "critic_count": new_count,
    }

    task_memory["critic_result"] = critic_result
    state["critic_result"] = critic_result

    # --- è·¯ç”±å†³ç­– ---
    routing = {
        "pass": "end",
        "revise": "retriever_node",
        "fail": "fail_answer_node",
    }[critic_result["status"]]

    # --- trace å†™å…¥ evidence-aware è¯­ä¹‰æ—¥å¿— ---
    append_trace(
        task_id=task_id,
        step="critic_node",
        tool="llm_semantic_critic",
        input_data={
            "query": user_query,
            "intent": intent,
            "hit_count": len(hits),
        },
        output={"critic_result": critic_result},
        next_step=routing,
    )

    save_checkpoint(task_id, state, "critic_node", routing)

    return state


def fail_answer_node(state: TaskState) -> TaskState:
    task_id = state["task_id"]
    task_memory = memory_store[task_id]

    critic = task_memory["critic_result"]
    reason = critic.get("reason", "unknown error")

    answer = "âš ï¸ å½“å‰æŸ¥è¯¢æœªèƒ½æˆåŠŸå¤„ç†ï¼ˆå·²ç»ˆæ­¢ï¼‰ã€‚\n" f"åŸå› ï¼š{reason}"

    state["answer"] = answer

    next_step = "end"

    append_trace(
        task_id=task_id,
        step="fail_answer_node",
        tool="system_fallback",
        input_data={"critic": critic},
        output={"answer": answer},
        status="warning",
        next_step=next_step,
    )

    save_checkpoint(task_id, state, last_step="fail_answer_node", next_step=next_step)

    return state


# ========= Trace-Driven Resume =========


def resume_from_checkpoint(app, task_id: str) -> Optional[TaskState]:
    ckpt = load_checkpoint(task_id)
    if not ckpt:
        print(f"âŒ no checkpoint for task {task_id}")
        return None

    print(f"ğŸ” Resuming from checkpoint for task: {task_id}")
    print(f"   last_step: {ckpt['last_step']}")
    print(f"   next_step: {ckpt['next_step']}")

    memory_store[task_id] = ckpt["memory"]
    state: TaskState = ckpt["state"]
    state["resume_next_step"] = ckpt["next_step"]

    result: TaskState = app.invoke(state)
    return result


# ========= æ„å»º Graph =========


def build_graph():
    graph = StateGraph(TaskState)

    graph.add_node("entry_node", entry_node)
    graph.add_node("planner_node", planner_node)
    graph.add_node("retriever_node", retriever_node)
    graph.add_node("executor_node", executor_node)
    graph.add_node("critic_node", critic_node)
    graph.add_node("fail_answer_node", fail_answer_node)

    graph.set_entry_point("entry_node")

    graph.add_conditional_edges(
        "entry_node",
        route_from_entry,
        {
            "planner_node": "planner_node",
            "retriever_node": "retriever_node",
            "executor_node": "executor_node",
            "critic_node": "critic_node",
            "fail_answer_node": "fail_answer_node",
            "end": END,
        },
    )

    graph.add_edge("planner_node", "retriever_node")
    graph.add_edge("retriever_node", "executor_node")
    graph.add_edge("executor_node", "critic_node")

    graph.add_conditional_edges(
        "critic_node",
        lambda s: {
            "pass": "end",
            "revise": "retriever_node",
            "fail": "fail_answer_node",
        }[s["critic_result"]["status"]],
        {
            "retriever_node": "retriever_node",
            "fail_answer_node": "fail_answer_node",
            "end": END,
        },
    )

    graph.add_edge("fail_answer_node", END)

    return graph.compile()


# ========= ä¸¤ä¸ªæµ‹è¯•ç”¨ä¾‹ =========


def print_trace(task_id: str, label: str):
    print(f"\n=== {label} Â· Execution Trace ===")
    for step in memory_store[task_id]["execution_trace"]:
        print(
            f"- step={step['step']} | tool={step['tool']} "
            f"| next={step.get('next_step')} | status={step['status']}"
        )


def test_high_relevance(app):
    print("\n================= æµ‹è¯• 1ï¼šé«˜ç›¸å…³æ–‡æ¡£ï¼ˆè®¢å•æŸ¥è¯¢ APIï¼‰ =================")
    task_id = init_task_memory()
    state = create_init_state(task_id, user_query="è¯·å¸®æˆ‘æŸ¥è¯¢è®¢å•æŸ¥è¯¢ API çš„æ¥å£è¯´æ˜")
    result = app.invoke(state)

    critic = result["critic_result"]
    print("\n[æµ‹è¯• 1] Critic ç»“æœï¼š")
    print(critic)
    print("\n[æµ‹è¯• 1] æœ€ç»ˆç­”æ¡ˆï¼š")
    print(result["answer"])
    # print_trace(task_id, "æµ‹è¯• 1")

    # é¢„æœŸï¼š
    # status = pass
    # reason = retrieval semantically matched task
    # action = None


def test_domain_mismatch(app):
    print(
        "\n================= æµ‹è¯• 2ï¼šæ•…æ„æ— å…³ queryï¼ˆé€€æ¬¾ UI layoutï¼‰ ================="
    )
    task_id = init_task_memory()
    state = create_init_state(task_id, user_query="è®¾è®¡ä¸€ä¸‹é€€æ¬¾æµç¨‹çš„ UI layout")
    result = app.invoke(state)

    critic = result["critic_result"]
    print("\n[æµ‹è¯• 2] Critic ç»“æœï¼š")
    print(critic)
    print("\n[æµ‹è¯• 2] æœ€ç»ˆç­”æ¡ˆï¼š")
    print(result["answer"])
    # print_trace(task_id, "æµ‹è¯• 2")

    # é¢„æœŸï¼š
    # status = revise
    # reason = evidence does not match task domain
    # action = redo_retriever
    # å¹¶ä¸” retriever ä¼šåœ¨ä¸‹ä¸€è½®è¢«é‡æ–°è°ƒç”¨ï¼ˆæœ‰ critic_count ä¸Šé™ä¿æŠ¤ï¼‰


if __name__ == "__main__":
    # å…ˆå¾€ Chroma é‡Œå¡ä¸€ç‚¹è®¢å• API æ–‡æ¡£ï¼Œç”¨ä½œå‘é‡æ£€ç´¢çš„çœŸå®åº•åº“
    sample_docs = [
        {
            "id": "orders-api-001",
            "title": "è®¢å•æŸ¥è¯¢æ¥å£",
            "text": """
GET /api/orders/{order_id}

å‚æ•°ï¼š
- order_id: è®¢å•ID

åŠŸèƒ½ï¼š
æ ¹æ®è®¢å•IDè¿”å›è®¢å•è¯¦æƒ…ï¼ŒåŒ…æ‹¬çŠ¶æ€ã€ä»·æ ¼ã€ç‰©æµä¿¡æ¯ã€‚
""",
        },
        {
            "id": "orders-api-002",
            "title": "è®¢å•åˆ—è¡¨æŸ¥è¯¢æ¥å£",
            "text": """
GET /api/orders?user_id={uid}

å‚æ•°ï¼š
- user_id: ç”¨æˆ·ID

åŠŸèƒ½ï¼š
è¿”å›ç”¨æˆ·æœ€è¿‘ 50 æ¡è®¢å•ï¼Œæ”¯æŒçŠ¶æ€ç­›é€‰ã€æ—¶é—´èŒƒå›´ç­›é€‰ã€‚
""",
        },
    ]

    vector_index.add_documents(sample_docs)

    app = build_graph()

    # æµ‹è¯• 1ï¼šé«˜ç›¸å…³ï¼ˆè®¢å•æŸ¥è¯¢ APIï¼‰
    # test_high_relevance(app)

    # æµ‹è¯• 2ï¼šæ•…æ„æ— å…³ï¼ˆé€€æ¬¾ UI layoutï¼‰
    test_domain_mismatch(app)
