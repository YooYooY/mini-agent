# ğŸ§  Mini-Agent

### A Hands-on ReACT + RAG + Multimodal Agent for Learning

---

## ğŸ“Œ Overview

**Mini-Agent** is a **hands-on learning project** that explores how modern LLM agents can be built using:

* **ReACT (Reason + Act)** as a control-flow pattern
* **Tool Calling** for external actions
* **RAG (Retrieval-Augmented Generation)** for knowledge retrieval
* **Multimodal input (text + image)**
* **FastAPI** as a simple backend interface

This project is **not production-ready**.
Its purpose is to provide a **clear, runnable reference** for understanding how these pieces work together in practice.

---

## âœ¨ What This Project Focuses On

* Understanding **ReACT as a control-flow design**, not a prompt format
* Seeing how **LLMs decide when to use tools**
* Treating **RAG as an optional information source**, not a magic database
* Integrating **multimodal inputs** into an agent loop
* Exposing an agent through a **simple HTTP API** for experimentation

---

## ğŸ§  Core Ideas

* **ReACT is about control flow**
  Reason â†’ Act â†’ Observe â†’ Decide again

* **Tool calling is structural, not textual**
  Tools are invoked via structured calls, not parsed from strings

* **RAG reduces uncertainty**
  The model does not know what is in the knowledge baseâ€”it decides when to retrieve

* **Multimodal input does not change the agent loop**
  Images are just another form of observation

---

## ğŸ—ï¸ High-level Architecture

```
Client (for testing)
    â†“
FastAPI endpoint
    â†“
ReACT Agent Loop
    â†“
LLM decides:
  - answer directly
  - or call a tool (RAG, etc.)
    â†“
Observation
    â†“
Final answer
```

---

## ğŸ“‚ Project Structure

```text
mini-agent/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ app.py        # FastAPI entry point
â”‚   â”œâ”€â”€ agent.py      # ReACT agent logic
â”‚   â”œâ”€â”€ rag.py        # RAG setup (vector store)
â”‚   â”œâ”€â”€ tools.py      # Tool definitions
â”œâ”€â”€ data/
â”‚   â””â”€â”€ langchain_intro.txt
â”œâ”€â”€ .env              # Environment variables
â””â”€â”€ requirements.txt
```

---

## âš™ï¸ Setup

### 1ï¸âƒ£ Create a virtual environment

```bash
python -m venv .venv
source .venv/bin/activate
```

### 2ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ Configure environment variables

Create a `.env` file in the project root:

```env
OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxx
```

---

## â–¶ï¸ Run the Server

From the project root:

```bash
uvicorn src.app:app --reload
```

Then visit:

```
http://localhost:8000/docs
```

---

## ğŸ”Œ API Example

### Endpoint

```http
POST /agent/ask
```

### Request

```json
{
  "query": "What is LangChain?",
  "image_url": "https://upload.wikimedia.org/wikipedia/commons/3/3a/Cat03.jpg"
}
```

### Response

```json
{
  "answer": "LangChain is a framework for building applications powered by large language models.",
  "used_tools": ["rag_search"],
  "status": "ok"
}
```

---

## ğŸ§© Notes on Agent Behavior

* The LLM decides whether to call tools based on:

  * The system prompt
  * Tool descriptions
  * The semantics of the question

* During tool calls, the model may return an empty `content` field.
  This is expected behavior when using structured tool calling.

---

## ğŸ” Why This Project Exists

Many tutorials focus on **prompt patterns** or **isolated features**.

This project was built to help answer questions like:

* How does an agent actually decide to use a tool?
* How do ReACT, RAG, and multimodal inputs fit into one loop?
* What does a minimal but realistic agent backend look like?

If you are learning AI application development or agent systems, this repository aims to be a **clear and honest reference**.

