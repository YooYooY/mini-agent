from dotenv import load_dotenv

load_dotenv()

import re
from typing import List

from pydantic import BaseModel
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings


# ============================================================
# 1. Business-level structured output (what your system needs)
# ============================================================


class Answer(BaseModel):
    """
    Final structured answer returned by the system.
    This is the BUSINESS contract.
    """

    answer: str
    sources: List[str]
    used_rag: bool


# ============================================================
# 2. Model-level structured output (what the LLM is good at)
# ============================================================


class RawRAGAnswer(BaseModel):
    """
    Raw semantic structure generated by the LLM.
    This matches how models naturally answer questions.
    """

    definition: str
    uses: List[str]


# ============================================================
# 3. Adapter: RawRAGAnswer -> Answer
# ============================================================


def adapt_raw_to_answer(
    raw: RawRAGAnswer,
    *,
    used_rag: bool,
    sources: List[str],
) -> Answer:
    """
    Adapter function.
    Converts model-level output into business-level output.
    """
    combined_answer = (
        raw.definition + "\n\nUses:\n" + "\n".join(f"- {u}" for u in raw.uses)
    )

    return Answer(
        answer=combined_answer,
        sources=sources,
        used_rag=used_rag,
    )


# ============================================================
# 4. Utility: extract pure JSON from LLM output
# ============================================================


def extract_json(text: str) -> str:
    """
    Remove markdown fences like ```json ... ``` if present.
    """
    text = text.strip()

    if text.startswith("```"):
        text = re.sub(r"^```[a-zA-Z]*\n?", "", text)
        text = re.sub(r"\n?```$", "", text)

    return text.strip()


# ============================================================
# 5. Build RAG retriever
# ============================================================


def build_retriever():
    loader = TextLoader("data/langchain_intro.txt")
    docs = loader.load()

    splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
    splits = splitter.split_documents(docs)

    embeddings = OpenAIEmbeddings()
    vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)

    return vectorstore.as_retriever(search_kwargs={"k": 2})


# ============================================================
# 6. Answer with RAG (LLM + Structured Semantic Output)
# ============================================================


def answer_with_rag(question: str, docs) -> Answer:
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "Return a JSON object with EXACTLY these fields:\n"
                "- definition: string\n"
                "- uses: array of strings\n"
                "Do NOT include markdown fences.",
            ),
            ("human", "Context:\n{context}\n\nQuestion:\n{question}"),
        ]
    )

    chain = prompt | llm

    context_text = "\n\n".join(d.page_content for d in docs)

    raw_text = chain.invoke({"question": question, "context": context_text}).content
    print("raw_text=>", raw_text)

    cleaned = extract_json(raw_text)
    print("cleaned=>", cleaned)

    # Parse model-level schema
    raw = RawRAGAnswer.model_validate_json(cleaned)

    # Adapt to business schema
    return adapt_raw_to_answer(raw, used_rag=True, sources=["langchain_intro.txt"])


# ============================================================
# 7. Main Agent Flow
# ============================================================


def main():
    retriever = build_retriever()

    question = "What is LangChain and what can it be used for?"

    docs = retriever.invoke(question)
    print("docs content=>", docs[0].page_content)

    result = answer_with_rag(question, docs)

    print("\n=== Final Structured Answer ===")
    print(result.model_dump())


if __name__ == "__main__":
    main()
